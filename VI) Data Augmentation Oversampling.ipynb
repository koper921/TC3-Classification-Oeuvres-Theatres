{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# on essaye de traiter dans ce notebook les classes que SMOTE n'a pas pu traiter \n",
    "### c est à dire créer de nouveaux documents synthétiques sans SMOTE, pour ensuite utliser SMOTE dessus \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Dans ce notebook on essaiera de traiter les cas que le SMOTE n a pas pu traiter <6, comme nous l avons vu precedement \n",
    "le smote a besoin d au moins 6 occurences dans le train par label pour pouvoir repliquer de nouveau doc synthétique\n",
    "donc dans ce notebook on s occupera des texte ayant des labels représenté de 2-3 à 10 fois (à voir )\n",
    "pour le reste peut etre qu on procedera par la suite à des découpages\n",
    "en fonction de la taille des textes (quand il y a qu une occurence par label ) pour \n",
    "appliquer des technique type SMOTE ou celle qu on va tacher d appliquer dans ce notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>0</th>\n",
       "      <th>genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>acte SCÈNE Argélie Clytie ARGÉLIE Gardes veni...</td>\n",
       "      <td>Tragédie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>acte SCÈNE PREMIÈRE Coriolan Albin CORIOLAN Q...</td>\n",
       "      <td>Tragédie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>acte SCÈNE Érigone Iphis IPHIS non Madame mor...</td>\n",
       "      <td>Tragédie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>SCÈNE PREMIÈRE Argante Nérine ARGANTE enfin N...</td>\n",
       "      <td>Comédie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>PAN ET DORIS SCÈNE premier PAN figurer Valémo...</td>\n",
       "      <td>Pastorale</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                                  0      genre\n",
       "0           0   acte SCÈNE Argélie Clytie ARGÉLIE Gardes veni...   Tragédie\n",
       "1           0   acte SCÈNE PREMIÈRE Coriolan Albin CORIOLAN Q...   Tragédie\n",
       "2           0   acte SCÈNE Érigone Iphis IPHIS non Madame mor...   Tragédie\n",
       "3           1   SCÈNE PREMIÈRE Argante Nérine ARGANTE enfin N...    Comédie\n",
       "4           2   PAN ET DORIS SCÈNE premier PAN figurer Valémo...  Pastorale"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_c.to_csv(\"body_base_notcut_clean_lemmatiser.csv\", sep='\\t')\n",
    "import pandas as pd \n",
    "df_c = pd.read_csv('body_base_notcut_clean_lemmatiser_notrand.csv',sep='\\t',  encoding = \"latin\" )\n",
    "df_c.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>genre</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>257</td>\n",
       "      <td>257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>444</td>\n",
       "      <td>444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>69</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       text  genre\n",
       "label             \n",
       "0       257    257\n",
       "1       444    444\n",
       "2         9      9\n",
       "3         8      8\n",
       "4         1      1\n",
       "5        11     11\n",
       "6        40     40\n",
       "7        13     13\n",
       "8         1      1\n",
       "9        23     23\n",
       "10        1      1\n",
       "11        2      2\n",
       "12       69     69\n",
       "13        3      3\n",
       "14       17     17\n",
       "15        7      7\n",
       "16        4      4\n",
       "17        1      1\n",
       "18        1      1\n",
       "19        2      2\n",
       "20        4      4\n",
       "21        2      2\n",
       "22        4      4\n",
       "23        3      3\n",
       "24        3      3\n",
       "25       10     10\n",
       "26       14     14\n",
       "27        4      4\n",
       "28        1      1\n",
       "29        1      1\n",
       "30        7      7\n",
       "31        1      1\n",
       "32        1      1\n",
       "33        1      1\n",
       "34        1      1\n",
       "35        5      5\n",
       "36        1      1\n",
       "37        6      6\n",
       "38        1      1\n",
       "39        4      4\n",
       "40        1      1\n",
       "41        2      2\n",
       "42        1      1\n",
       "43        2      2\n",
       "44        2      2\n",
       "45        6      6\n",
       "46       19     19\n",
       "47        1      1\n",
       "48        1      1\n",
       "49        1      1\n",
       "50        1      1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_c.columns= ['label', 'text', 'genre']\n",
    "df_c.groupby(['label']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>acte SCÈNE PREMIÈRE Lisimène Bélise LISIMÈNE ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>prologue Le théâtre représenter jardin ordonn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>prologue PAYSAN conduisant troupe paysan pays...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5</td>\n",
       "      <td>acte premier tableau Au lever rideau théâtre ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>10</td>\n",
       "      <td>prologue LA NYMPHE Dieu Parnasse sacré vallon...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    label                                               text\n",
       "0       0   acte SCÈNE PREMIÈRE Lisimène Bélise LISIMÈNE ...\n",
       "1       1   prologue Le théâtre représenter jardin ordonn...\n",
       "6       1   prologue PAYSAN conduisant troupe paysan pays...\n",
       "10      5   acte premier tableau Au lever rideau théâtre ...\n",
       "32     10   prologue LA NYMPHE Dieu Parnasse sacré vallon..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_6_i =df_c.groupby(['label']).count()<11\n",
    "index = list(df_6_i.loc[df_6_i['text']==True].index)\n",
    "df_6 = df_c.loc[df_c.label.isin(index) ]\n",
    "\n",
    "df_6.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traducteurs utilisés pour la méthode synonyme\n",
    "#### plus aucun traducteurs ne marche google a bloqué l'adresse Ip à cause des requetes de traduction trop importante\n",
    "#### sur googletrans , code qui a subitement arreté de marché en renvoyant une jsonerror\n",
    "## un résultat de traduction / détraduction en bas + méthode naive / methode naive + tdfidif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  04 HOURS 52 MINUTES 19 SECONDSVISIT HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP TO TRANSLATE MORE'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from translate import Translator\n",
    "translator= Translator(from_lang=\"french\",to_lang=\"english\")\n",
    "texte = df_6.text.iloc[0][0:480]\n",
    "translation = translator.translate(texte)\n",
    "translator1= Translator(from_lang=\"english\",to_lang=\"french\")\n",
    "translator1.translate(translation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-f57b54f55359>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgoogletrans\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTranslator\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtranslator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTranslator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mtranslator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'bonjour'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Programmes\\Anaconda2\\lib\\site-packages\\googletrans-2.3.0-py3.6.egg\\googletrans\\client.py\u001b[0m in \u001b[0;36mtranslate\u001b[1;34m(self, text, dest, src)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m         \u001b[0morigin\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 172\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_translate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m         \u001b[1;31m# this code will be updated when the format is changed.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programmes\\Anaconda2\\lib\\site-packages\\googletrans-2.3.0-py3.6.egg\\googletrans\\client.py\u001b[0m in \u001b[0;36m_translate\u001b[1;34m(self, text, dest, src)\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat_json\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programmes\\Anaconda2\\lib\\site-packages\\googletrans-2.3.0-py3.6.egg\\googletrans\\utils.py\u001b[0m in \u001b[0;36mformat_json\u001b[1;34m(original)\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moriginal\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m         \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlegacy_format_json\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moriginal\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mconverted\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programmes\\Anaconda2\\lib\\site-packages\\googletrans-2.3.0-py3.6.egg\\googletrans\\utils.py\u001b[0m in \u001b[0;36mlegacy_format_json\u001b[1;34m(original)\u001b[0m\n\u001b[0;32m     52\u001b[0m             \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstates\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnxt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mconverted\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programmes\\Anaconda2\\lib\\json\\__init__.py\u001b[0m in \u001b[0;36mloads\u001b[1;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    352\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    353\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[1;32m--> 354\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    355\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m         \u001b[0mcls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programmes\\Anaconda2\\lib\\json\\decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    338\u001b[0m         \"\"\"\n\u001b[1;32m--> 339\u001b[1;33m         \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    340\u001b[0m         \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programmes\\Anaconda2\\lib\\json\\decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    355\u001b[0m             \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 357\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Expecting value\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    358\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "# Code qui ne marche plus subitement surement problème de gooogle ... d'autres erreurs 403 sur d'autres traducteurs\n",
    "# très instable\n",
    "# Confirmation : IP bloqué pour les traductions quand trop de requêtes ....\n",
    "\n",
    "from googletrans import Translator\n",
    "translator = Translator()\n",
    "translator.translate('bonjour')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0     LONGUEUR DE LA REQUÊTE LIMITE EXCEDEED. MAX R...\n",
       "0     MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE...\n",
       "0     MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE...\n",
       "0     MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE...\n",
       "0     MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE...\n",
       "0     MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE...\n",
       "0     MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE...\n",
       "0     MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE...\n",
       "0     MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE...\n",
       "dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean.loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_10_5 = df_6_i "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Vectorisation tfidif ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Body not cut , body en ENTIER ici\n",
    "\n",
    "#on transforme la df en Xtrain, X_test, y_test, y_train ...\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "y = df_c.iloc[:,0]\n",
    "\n",
    "\n",
    "list_text = df_c.iloc[:,1]\n",
    "list_label = df_c.iloc[:,0]\n",
    "\n",
    "\n",
    "\n",
    "test_size = int(len(list_text)/5)\n",
    "test_set , train_set  = list_text[:test_size], list_text[test_size:]\n",
    "test_label, train_label = list_label[:test_size], list_label[test_size:]\n",
    "\n",
    "#Oversampling : pour essayer de mieux prédire les classes sous représentées \n",
    "count_vect = CountVectorizer()\n",
    "X = count_vect.fit_transform(list_text)\n",
    "X_test, X_train = X[:test_size, :], X[test_size:, :]\n",
    "y_train, y_test = train_label, test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 2 1]]\n",
      "[[1 2 2 1]]\n"
     ]
    }
   ],
   "source": [
    "# Puisque nous vectorisons au niveau du mot l'odre n'a pas d'importance, de même shuffler un document pour en créer un nouveau\n",
    "# ne servirait à rien puisqu'il répliquer exactement le même au niveau vectoriel ou peut d'être de l'overfitting\n",
    "count_vect =CountVectorizer()\n",
    "texte= \"salut tous le monde salut monde\"\n",
    "texte1 = \"monde le salut salut monde tous \"\n",
    "print(count_vect.fit_transform([texte]).todense())\n",
    "print(count_vect.fit_transform([texte1]).todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Méthode naive , on fait une moyenne de documents, en piochant de manière aléatoire un nombre dans la taille du vocabulaire \n",
    "renvoit une liste de document synthétique, mauvais résultat au sens similarité cosin "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Strategie 1 : \"Moyenne de document\" on shuffle entre plusieurs doc de meme label , le principe est simple on tire au hasard des\n",
    "# entiers  dans le vocabulaire (les matrices countVectorizer) pour obtenir des mots et on fait en sorte  que la taille \n",
    "# du nouveau doc respecte la moyenne des autres \n",
    "#On verra par la suite si on ajoute la distribution moyenne des tag (Nom, ADJ) selon les résultat de cette première méthode \n",
    "# On verifiera la qualité du nouveau document créer avec la similarité cosin avec ces documents parents \n",
    "\n",
    "import random\n",
    "random.randint(100,500)\n",
    "\n",
    "ex = df_c[df_c.label == 25] # 4 occurences pour label 25 \n",
    "def mean_doc(list_doc,size):\n",
    "    count_vect = CountVectorizer()\n",
    "    X = count_vect.fit_transform(list_doc)\n",
    "    new_doc = np.zeros((size, X.shape[1]) ) #+ int(1/3*np.max(np.sum(X, axis=1)))))\n",
    "    for j in range(size):\n",
    "        Max =  np.mean(np.sum(X, axis = 1))+ random.randint(- int(1/3*np.min(np.sum(X,axis=1))), int(1/3*np.max(np.sum(X, axis=1))))\n",
    "        Max = int(Max)\n",
    "        for i  in range(Max) :  #+ random_variable taille :\n",
    "\n",
    "            rand1 = random.randint(0,len(list_doc)-1)\n",
    "            rand2 = random.randint(0,X.shape[1]-1)\n",
    "            rand3= random.randint(0, X.shape[1]-1) #+int(1/3*np.max(np.sum(X, axis=1))))\n",
    "           #print(rand1, rand2, rand3)\n",
    "            #print(new_doc.shape)\n",
    "           # print(X.shape)\n",
    "            new_doc[j, rand3]= X[rand1, rand2]\n",
    "    return new_doc\n",
    "\n",
    "new_doc = mean_doc(list(ex.text), 4)\n",
    "        \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Résultat méthode naive : similarité cosin très faible "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.01556289]\n",
      " [0.03162771]\n",
      " [0.02645662]]\n"
     ]
    }
   ],
   "source": [
    "# Methode shuffle ne marche pas il ffaudrait ajouter des poids selon l appraition des mots ou non car on peut facilement\n",
    "# perdre des mots clé avec cette méthode et donc des similarités cosine très faible ...\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "X_train_counts = new_doc\n",
    "test_set, train_set = X_train_counts[0,:], X_train_counts[1:,:]\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "tfidf_matrix_train = tfidf_transformer.fit_transform(train_set)\n",
    "tfidf_matrix_test = tfidf_transformer.transform([test_set])\n",
    "\n",
    "print(cosine_similarity(tfidf_matrix_train,tfidf_matrix_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Méthode naive modifiée, on continue la boucle tant que valeur sum tfidif moyenne n'est pas atteinte\n",
    "on essaye ici de capter en plus les mots important, qui on du sens , suceptible de faire augmenté la similarité des documents : \n",
    "résultat pas bon non plus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "random.randint(100,500)\n",
    "\n",
    "ex = df_c[df_c.label == 25] \n",
    "print(len(ex) )# 4 occurences pour label 25 \n",
    "def mean_doc(list_doc,size):\n",
    "    count_vect = CountVectorizer()\n",
    "    X = count_vect.fit_transform(list_doc)\n",
    "    print(type(X))\n",
    "    tfidf = TfidfTransformer()\n",
    "    list_xid =[]\n",
    "    print(X.shape)\n",
    "    for i in range(X.shape[0]) :  # on sépare un par un par pour pas que les noms des personnage prennent trop d'importance (réduit idf)\n",
    "        X_tfidf = tfidf.fit_transform(X[i, :] )\n",
    "        \n",
    "        list_xid.append(X_tfidf.todense().tolist())\n",
    "    array = np.concatenate(list_xid)\n",
    "    print(array[0,:] )\n",
    "    moyenne = 4* np.mean(np.sum(array,axis=1))\n",
    "    print(moyenne)\n",
    "    print(np.sum(array[1]))\n",
    "    new_doc = np.zeros((size, X.shape[1]) )\n",
    "    sum_tfidf = 0\n",
    "    while sum_tfidf < moyenne:\n",
    "\n",
    "         #+ int(1/3*np.max(np.sum(X, axis=1)))))\n",
    "        for j in range(size):\n",
    "            Max =  np.mean(np.sum(X, axis = 1))+ random.randint(- int(1/3*np.min(np.sum(X,axis=1))), int(1/3*np.max(np.sum(X, axis=1))))\n",
    "            Max = int(Max)\n",
    "            for i  in range(Max) :  #+ random_variable taille :\n",
    "\n",
    "                rand1 = random.randint(0,len(list_doc)-1)\n",
    "                rand2 = random.randint(0,X.shape[1]-1)\n",
    "                rand3= random.randint(0, X.shape[1]-1) #+int(1/3*np.max(np.sum(X, axis=1))))\n",
    "               #print(rand1, rand2, rand3)\n",
    "                #print(new_doc.shape)\n",
    "               # print(X.shape)\n",
    "                new_doc[j, rand3]= new_doc[j, rand3] + X[rand1, rand2]\n",
    "                sum_tfidf = sum_tfidf + array[rand1, rand2]\n",
    "    return new_doc\n",
    "\n",
    "new_doc = mean_doc(list(ex.text), 4)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calcul de similarité cosin entre meme label (à titre d'exemple )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#train_set = [\"president of India\",\"machine learning is awesome\", \"python is awesome\", \"thanks for reading\"]\n",
    "x_set = list(df_c[df_c.label == 25].text)\n",
    "\n",
    "test_set, train_set = x_set[0], x_set[1:]\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix_train = tfidf_vectorizer.fit_transform(train_set)\n",
    "tfidf_matrix_test = tfidf_vectorizer.transform([test_set])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.12903819]\n",
      " [0.3198419 ]\n",
      " [0.29568561]]\n"
     ]
    }
   ],
   "source": [
    "#On pourrez penser que la similarité cosin soit plus elevée entre texte de meme label mais non\n",
    "# d'autres paramètres doivent rentrer en jeu comme l'auteur, l epoque => pas forcement bonne évaluation pour notre\n",
    "# méthode de creation de document \n",
    "print(cosine_similarity(tfidf_matrix_train,tfidf_matrix_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.25829566]\n",
      " [0.37447826]\n",
      " [0.25107237]\n",
      " [0.35208983]]\n"
     ]
    }
   ],
   "source": [
    "x_set = list(df_c.text.iloc[25:30])\n",
    "test_set, train_set = x_set[0], x_set[1:]\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix_train = tfidf_vectorizer.fit_transform(train_set)\n",
    "tfidf_matrix_test = tfidf_vectorizer.transform([test_set])\n",
    "print(cosine_similarity(tfidf_matrix_train,tfidf_matrix_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Les fonctions synonymes et batch, pour traduire détraduire\n",
    "on batch pour passer le texte dans le traducteurs sinon trop gros "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.33777743]\n",
      " [0.60618773]\n",
      " [0.56291769]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from googletrans import Translator\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "x_set = list(df_c[df_c.label == 25].text)\n",
    "\n",
    "# On creer une fonction batch pour pour pouvoir traduire les texte , google translate a un nombre limité de charactères\n",
    "def batch(text):\n",
    "    list_mot= text.split()\n",
    "    s_list=[]\n",
    "    compt = 0\n",
    "    s=[]\n",
    "    for mot  in list_mot:\n",
    "        compt= compt + len(mot)\n",
    "        if compt <2000:\n",
    "            s.append(mot)\n",
    "        else : \n",
    "            compt= 0\n",
    "            s_list.append(s)\n",
    "            s=[]\n",
    "            s.append(mot)\n",
    "    \n",
    "    list_string=[]\n",
    "    for l in s_list:\n",
    "        s=\"\"\n",
    "        for word in l :\n",
    "            s= s+ \" \"+word\n",
    "        list_string.append(s)\n",
    "        \n",
    "    return list_string\n",
    "\n",
    "def synonime(list_text):\n",
    "    translator = Translator()\n",
    "    new_doc=[]\n",
    "    i=0\n",
    "    for element in x_set :\n",
    "        #print(i)\n",
    "        s=\"\"\n",
    "        batch_text  =batch(element)\n",
    "        for b in batch_text:\n",
    "            \n",
    "            trans = translator.translate(b)\n",
    "            trans2= translator.translate(trans.text, dest ='fr')\n",
    "            s= s+ \" \" +trans2.text\n",
    "        i=i+1\n",
    "        new_doc.append(s)\n",
    "    return new_doc\n",
    "\n",
    "\n",
    "# la tactique de la traduction introduit plus de similarité cosin que precedement , \n",
    "# elements de reponse un traducteur vs plusieurs  style d'écrivains => un vocabulaire plus simple par traduction/ détraduction\n",
    "# initialement similatrité cosin [[0.12903819] [0.3198419 ][0.29568561]] entre 4 documents originaux \n",
    "    \n",
    "new_doc = synonime(x_set)\n",
    "test_set, train_set = new_doc[0], new_doc[1:]\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix_train = tfidf_vectorizer.fit_transform(train_set)\n",
    "tfidf_matrix_test = tfidf_vectorizer.transform([test_set])\n",
    "print(cosine_similarity(tfidf_matrix_train,tfidf_matrix_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 73% de similarité cosin du texte traduit/ détraduit avec le texte original \n",
    "=> bon résultat, car pas totalement similaire (pas de redondances ), mais proche quand même"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "[[0.11105139]\n",
      " [0.27703567]\n",
      " [0.25435021]\n",
      " [0.73683557]\n",
      " [0.14084327]\n",
      " [0.30589845]\n",
      " [0.24389524]]\n"
     ]
    }
   ],
   "source": [
    "#verifions entre les 8 documents maintenant \n",
    "# le document a 0.73 de similarité cosin avec sa réplique, conserve les propotion de similarité cosin originaux et texte créer \n",
    "# => résultat encouragant pour dupliquer les textes * 2 \n",
    "# à voir si l'introduction potentiel de nouveau vocabulaire ne réduit pas la précision du modèle \n",
    "# si cette méthode marche on va pouvoir traiter les cas entre 5 et 10 d'occurence de label  \n",
    "# mais il faudra au préalable relemmatizer le texte traduit puis détraduit \n",
    "\n",
    "new_doc = x_set +synonime(x_set) \n",
    "test_set, train_set = new_doc[0], new_doc[1:]\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix_train = tfidf_vectorizer.fit_transform(train_set)\n",
    "tfidf_matrix_test = tfidf_vectorizer.transform([test_set])\n",
    "print(cosine_similarity(tfidf_matrix_train,tfidf_matrix_test))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
